"""
Service de gestion des blobs Azure Storage
Adapt√© de la logique Azure Functions existante
"""

import logging
import base64
import os
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, List
from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)


class BlobService:
    """Service pour la gestion des blobs Azure Storage"""

    def __init__(self):
        # Configuration Azure Storage
        self.account_name = os.getenv('AZURE_ACCOUNT_NAME')
        self.account_key = os.getenv('AZURE_ACCOUNT_KEY')

        if not self.account_key.endswith("=="):
            self.account_key += "=="

        # Noms des conteneurs
        self.input_container = "doc-to-trad"
        self.output_container = "doc-trad"

        # Client Blob Storage
        self.blob_service_client = BlobServiceClient(
            account_url=f"https://{self.account_name}.blob.core.windows.net",
            credential=self.account_key
        )

        logger.info("‚úÖ BlobService initialis√©")

    async def prepare_blobs(self, file_content_base64: str, file_name: str, target_language: str) -> Dict[str, str]:
        """
        Pr√©pare les blobs source et cible pour la traduction
        √âquivalent de la fonction prepare_blobs d'Azure Functions
        """

        logger.info(
            f"üîÑ Pr√©paration des blobs pour {file_name} ‚Üí {target_language}")

        try:
            # Normalisation des noms de fichiers pour Azure Storage
            normalized_file_name = self._normalize_blob_name(file_name)

            # G√©n√©ration des noms de fichiers avec suffixe de langue am√©lior√©
            input_blob_name = normalized_file_name
            file_base, file_ext = normalized_file_name.rsplit(
                ".", 1) if "." in normalized_file_name else (normalized_file_name, "")

            # Format am√©lior√©: file_name-fr.docx au lieu de file_name_fr.docx
            output_blob_name = f"{file_base}-{target_language}.{file_ext}" if file_ext else f"{file_base}-{target_language}"

            logger.info(f"üìÑ Fichier source: {input_blob_name}")
            logger.info(f"üìÑ Fichier cible: {output_blob_name}")

            # Nettoyage des anciens fichiers (>1h)
            await self._delete_old_files(self.output_container, max_age_hours=1)

            # Suppression du fichier cible s'il existe d√©j√†
            await self._check_and_delete_target_blob(self.output_container, output_blob_name)

            # Conversion et upload du fichier source
            file_content_binary = base64.b64decode(file_content_base64)

            input_blob_client = self.blob_service_client.get_blob_client(
                container=self.input_container,
                blob=input_blob_name
            )
            input_blob_client.upload_blob(file_content_binary, overwrite=True)
            logger.info("‚úÖ Fichier source upload√©")

            # G√©n√©ration des SAS URLs
            source_url = self._generate_sas_url(
                self.input_container, input_blob_name, permissions="r")
            target_url = self._generate_sas_url(
                self.output_container, output_blob_name, permissions="rw")

            logger.info("‚úÖ URLs SAS g√©n√©r√©es")

            return {
                "source_url": source_url,
                "target_url": target_url,
                "input_blob_name": input_blob_name,
                "output_blob_name": output_blob_name,
                "original_file_name": file_name,
                "normalized_file_name": normalized_file_name
            }

        except Exception as e:
            logger.error(
                f"‚ùå Erreur lors de la pr√©paration des blobs: {str(e)}")
            raise

    async def download_translated_file(self, target_url: str) -> bytes:
        """T√©l√©charge le fichier traduit depuis le blob storage"""

        try:
            # Extraction du nom du blob depuis l'URL
            # Format: https://account.blob.core.windows.net/container/blob?sas
            url_parts = target_url.split('/')
            container_name = url_parts[-2]
            blob_name_with_sas = url_parts[-1]
            blob_name = blob_name_with_sas.split('?')[0]

            logger.info(f"üì• T√©l√©chargement du fichier: {blob_name}")

            blob_client = self.blob_service_client.get_blob_client(
                container=container_name,
                blob=blob_name
            )

            if not blob_client.exists():
                raise FileNotFoundError(
                    f"Le fichier traduit {blob_name} n'existe pas dans le conteneur {container_name}")

            download_stream = blob_client.download_blob()
            content = download_stream.readall()

            logger.info(f"‚úÖ Fichier traduit t√©l√©charg√©: {len(content)} bytes")
            return content

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du t√©l√©chargement: {str(e)}")
            # Log plus d√©taill√© pour d√©boguer
            if 'InvalidResourceName' in str(e):
                logger.error(f"üîç URL probl√©matique: {target_url}")
                logger.error(
                    f"üîç Container: {container_name if 'container_name' in locals() else 'N/A'}")
                logger.error(
                    f"üîç Blob: {blob_name if 'blob_name' in locals() else 'N/A'}")
            raise Exception(f"Erreur lors de la r√©cup√©ration: {str(e)}")

    def _generate_sas_url(self, container_name: str, blob_name: str, permissions: str = "r") -> str:
        """G√©n√®re une URL SAS pour un blob"""

        # Configuration des permissions
        perm_map = {
            "r": BlobSasPermissions(read=True),
            "w": BlobSasPermissions(write=True),
            "rw": BlobSasPermissions(read=True, write=True)
        }

        sas_token = generate_blob_sas(
            account_name=self.account_name,
            container_name=container_name,
            blob_name=blob_name,
            account_key=self.account_key,
            permission=perm_map.get(
                permissions, BlobSasPermissions(read=True)),
            expiry=datetime.utcnow() + timedelta(hours=2)  # 2h de validit√©
        )

        blob_client = self.blob_service_client.get_blob_client(
            container=container_name,
            blob=blob_name
        )

        return f"{blob_client.url}?{sas_token}"

    async def _delete_old_files(self, container_name: str, max_age_hours: int = 1):
        """Supprime les fichiers anciens du conteneur"""

        try:
            container_client = self.blob_service_client.get_container_client(
                container_name)
            blob_list = container_client.list_blobs()
            cutoff_time = datetime.utcnow().replace(tzinfo=timezone.utc) - \
                timedelta(hours=max_age_hours)

            deleted_count = 0
            for blob in blob_list:
                if blob.last_modified < cutoff_time:
                    container_client.delete_blob(blob.name)
                    deleted_count += 1
                    logger.info(f"üóëÔ∏è Fichier supprim√©: {blob.name}")

            if deleted_count > 0:
                logger.info(
                    f"‚úÖ {deleted_count} ancien(s) fichier(s) supprim√©(s)")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors du nettoyage: {str(e)}")

    async def _check_and_delete_target_blob(self, container_name: str, blob_name: str):
        """Supprime le fichier cible s'il existe pour √©viter les conflits"""

        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container_name,
                blob=blob_name
            )

            if blob_client.exists():
                blob_client.delete_blob()
                logger.info(f"üóëÔ∏è Fichier cible existant supprim√©: {blob_name}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur lors de la suppression: {str(e)}")

    async def check_blob_exists(self, container: str, blob_name: str) -> bool:
        """V√©rifie si un blob existe dans un container"""
        try:
            blob_client = self.blob_service_client.get_blob_client(
                container=container,
                blob=blob_name
            )
            return blob_client.exists()
        except Exception as e:
            logger.error(
                f"Erreur lors de la v√©rification du blob {blob_name}: {str(e)}")
            return False

    async def prepare_translation_urls(self, input_blob_name: str, target_language: str) -> Dict[str, str]:
        """
        Pr√©pare les URLs pour la traduction d'un blob existant
        Le fichier source est d√©j√† dans le container doc-to-trad
        """

        logger.info(
            f"üîÑ Pr√©paration des URLs pour {input_blob_name} ‚Üí {target_language}")

        try:
            # G√©n√©ration du nom du fichier de sortie
            file_base, file_ext = input_blob_name.rsplit(
                ".", 1) if "." in input_blob_name else (input_blob_name, "")

            # Format: file_name-fr.docx
            output_blob_name = f"{file_base}-{target_language}.{file_ext}" if file_ext else f"{file_base}-{target_language}"

            logger.info(f"üìÑ Fichier source: {input_blob_name}")
            logger.info(f"üìÑ Fichier cible: {output_blob_name}")

            # Nettoyage des anciens fichiers (>1h)
            await self._delete_old_files(self.output_container, max_age_hours=1)

            # Suppression du fichier cible s'il existe d√©j√†
            await self._check_and_delete_target_blob(self.output_container, output_blob_name)

            # G√©n√©ration des SAS URLs
            source_url = self._generate_sas_url(
                self.input_container, input_blob_name, permissions="r")
            target_url = self._generate_sas_url(
                self.output_container, output_blob_name, permissions="rw")

            logger.info("‚úÖ URLs SAS g√©n√©r√©es")

            return {
                "source_url": source_url,
                "target_url": target_url,
                "input_blob_name": input_blob_name,
                "output_blob_name": output_blob_name,
                "original_file_name": input_blob_name,
                "normalized_file_name": input_blob_name
            }

        except Exception as e:
            logger.error(f"Erreur lors de la pr√©paration des URLs: {str(e)}")
            raise

    def _normalize_blob_name(self, file_name: str) -> str:
        """
        Normalise un nom de fichier pour √™tre compatible avec Azure Storage
        - Remplace les espaces par des underscores
        - Remplace les caract√®res sp√©ciaux par des underscores
        - Supprime les caract√®res cons√©cutifs d'underscores
        """
        import re

        # Remplacer les espaces et tirets par des underscores
        normalized = file_name.replace(' ', '_').replace('-', '_')

        # Garder seulement les caract√®res alphanum√©riques, points et underscores
        normalized = re.sub(r'[^a-zA-Z0-9._]', '_', normalized)

        # Supprimer les underscores cons√©cutifs
        normalized = re.sub(r'_+', '_', normalized)

        # Supprimer les underscores en d√©but et fin
        normalized = normalized.strip('_')

        # Limiter la longueur (garder extension si pr√©sente)
        max_length = 200  # Limite conservatrice pour √©viter les probl√®mes
        if len(normalized) > max_length:
            if '.' in normalized:
                name_part, ext = normalized.rsplit('.', 1)
                max_name_length = max_length - len(ext) - 1  # -1 pour le point
                normalized = name_part[:max_name_length] + '.' + ext
            else:
                normalized = normalized[:max_length]

            logger.warning(
                f"‚ö†Ô∏è Nom de fichier tronqu√©: {file_name} ‚Üí {normalized}")

        logger.info(f"üìù Nom normalis√©: '{file_name}' ‚Üí '{normalized}'")
        return normalized

    async def list_input_files(self, filter_extension: str = None) -> List[str]:
        """Liste les fichiers dans le container d'entr√©e"""
        try:
            container_client = self.blob_service_client.get_container_client(
                self.input_container
            )

            files = []
            # Utilisation de la m√©thode synchrone avec list_blobs()
            for blob in container_client.list_blobs():
                blob_name = blob.name

                # Filtrage par extension si sp√©cifi√©
                if filter_extension:
                    if not blob_name.lower().endswith(f".{filter_extension.lower()}"):
                        continue

                files.append(blob_name)

            return sorted(files)  # Tri alphab√©tique

        except Exception as e:
            logger.error(f"Erreur lors de la liste des fichiers: {str(e)}")
            return []

    def list_output_files(self, filter_extension: str = None) -> List[Dict[str, Any]]:
        """Liste les fichiers dans le container de sortie (doc-trad) avec leurs URLs"""
        try:
            container_client = self.blob_service_client.get_container_client(
                self.output_container
            )

            files = []
            # Utilisation de la m√©thode synchrone avec list_blobs()
            for blob in container_client.list_blobs():
                blob_name = blob.name

                # Filtrage par extension si sp√©cifi√©
                if filter_extension:
                    if not blob_name.lower().endswith(f".{filter_extension.lower()}"):
                        continue

                # G√©n√©ration de l'URL SAS pour le fichier
                try:
                    sas_url = self._generate_sas_url(
                        self.output_container, blob_name, permissions="r")

                    files.append({
                        "name": blob_name,
                        "size": blob.size,
                        "last_modified": blob.last_modified.isoformat() if blob.last_modified else None,
                        "url": sas_url,
                        "content_type": blob.content_settings.content_type if blob.content_settings else None
                    })
                except Exception as url_error:
                    logger.error(
                        f"Erreur g√©n√©ration URL pour {blob_name}: {str(url_error)}")
                    files.append({
                        "name": blob_name,
                        "size": blob.size,
                        "last_modified": blob.last_modified.isoformat() if blob.last_modified else None,
                        "url": None,
                        "error": str(url_error)
                    })

            return sorted(files, key=lambda x: x["name"])  # Tri alphab√©tique

        except Exception as e:
            logger.error(
                f"Erreur lors de la liste des fichiers de sortie: {str(e)}")
            return []

    def get_output_file_url(self, blob_name, expires_in_hours=24):
        """
        G√©n√®re une URL SAS pour un fichier dans le container de sortie (doc-trad)
        """
        try:
            container_client = self.blob_service_client.get_container_client(
                "doc-trad")
            blob_client = container_client.get_blob_client(blob_name)

            # V√©rifier si le blob existe
            if not blob_client.exists():
                return None

            # G√©n√©rer l'URL SAS
            sas_token = generate_blob_sas(
                account_name=self.account_name,
                container_name="doc-trad",
                blob_name=blob_name,
                account_key=self.account_key,
                permission=BlobSasPermissions(read=True),
                expiry=datetime.utcnow() + timedelta(hours=expires_in_hours)
            )

            url = f"https://{self.account_name}.blob.core.windows.net/doc-trad/{blob_name}?{sas_token}"
            return url

        except Exception as e:
            print(
                f"Erreur lors de la g√©n√©ration de l'URL pour {blob_name}: {e}")
            return None
